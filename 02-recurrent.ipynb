{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequences\n",
    "\n",
    "A simple example of sequence prediction - $[0, 1, 2] \\rightarrow [3, 4, 5]$\n",
    "\n",
    "Application examples:\n",
    "![](assets/application_examples.jpg)\n",
    "\n",
    "We can have different types of sequence problem structures\n",
    "\n",
    "![](assets/sequences.png)\n",
    "\n",
    "The many to many structure can also be thought of as an encoder-decoder structure:\n",
    "\n",
    "![](assets/quoc-le.png)\n",
    "\n",
    "## Problems with standard dense networks\n",
    "\n",
    "+ Fixed size inputs & outputs\n",
    "+ Stateless\n",
    "+ Doesn't share features learned across positions\n",
    "+ Unaware of temporal structure\n",
    "\n",
    "\n",
    "## Promise of recurrent neural networks\n",
    "\n",
    "Network able to learn a mapping from inputs over time\n",
    "- outputs become conditional the context of the sequence\n",
    "\n",
    "Learn the temporal dependence of data\n",
    "\n",
    "An RRN is Turing complete\n",
    "- they can simulate arbitrary programs\n",
    "\n",
    "## Being comfortable in three dimensions\n",
    "\n",
    "We model the temporal structure in data using a dimension in an array - by convention this is the second dimension.\n",
    "\n",
    "Our dimensions then are: \n",
    "- $m$ = the batch dimension (number of samples)\n",
    "- $T_x$ = timesteps (length of sequence)\n",
    "- $n_x$ = features at each time-step\n",
    "\n",
    "## Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:50:01.190888Z",
     "start_time": "2019-09-30T15:49:56.813545Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 32, 16])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow import random\n",
    "random.set_seed(3) # for reproducible results\n",
    "\n",
    "\n",
    "m = 1000\n",
    "T_x = 32\n",
    "n_x = 16\n",
    "\n",
    "samples = tf.random.uniform((m, T_x, n_x))\n",
    "\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select all samples, first timestep, all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.numpy()[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last sample, all timesteps, first feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.numpy()[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ninth sample, sixth timestep, all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.numpy()[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "A recurrent neural network is a linear stack of the one same model. It passes its output to itself at each timestep.\n",
    "![](assets/RNN.png)\n",
    "\n",
    "### RNN cell\n",
    "![](assets/rnn_step_forward.png)\n",
    "\n",
    "Basic RNN cell. Takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (activation value or previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $y^{\\langle t \\rangle}$\n",
    "\n",
    "### RNN forward pass \n",
    "\n",
    "You can see an RNN as the repetition of the cell you've just built. If your input sequence of data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell ($a^{\\langle t-1 \\rangle}$) and the current time-step's input data ($x^{\\langle t \\rangle}$). It outputs a new hidden state ($a^{\\langle t \\rangle}$) and a prediction ($y^{\\langle t \\rangle}$) for this time-step.\n",
    "\n",
    "\n",
    "![](assets/rnn.png)\n",
    "Basic RNN. The input sequence $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  is carried over $T_x$ time steps. The network outputs $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical \n",
    "\n",
    "Let's code a forward propagation similar to the RNN described in the figure above, but simpler, that is without biases.\n",
    "We also want to have one output for each time step, that means  $T_x$ =  $T_y$\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Create a random input vector x with 4 samples, 3 time-steps and 2 features at each time-step  \n",
    "+ Initialize the architecture and weights with hidden size = 4 \n",
    "+ Create the hidden state vectors ($a$) as a vector of zeros that will store the values computer by RNN\n",
    "+ Loop over each time-step (index t)\n",
    "    + Calculate the \"next\" hidden state using tanh as activation function\n",
    "    + Calculate the prediciton y in this time-step\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.88571262, 0.59926573],\n",
       "        [0.47102437, 0.55445859],\n",
       "        [0.44981887, 0.09304059]],\n",
       "\n",
       "       [[0.81935112, 0.90132869],\n",
       "        [0.47578489, 0.96259905],\n",
       "        [0.82384898, 0.45511733]],\n",
       "\n",
       "       [[0.01477745, 0.35827748],\n",
       "        [0.37231846, 0.68358015],\n",
       "        [0.32437978, 0.41808654]],\n",
       "\n",
       "       [[0.56037974, 0.02307087],\n",
       "        [0.71701891, 0.03027807],\n",
       "        [0.20014658, 0.7836883 ]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random input vector x with m samples,T_x time-steps and n_x features\n",
    "m = \n",
    "T_x = \n",
    "n_x = \n",
    "\n",
    "x = np.random.uniform(size=(m, T_x, n_x))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xt  -- your input data at timestep \"t\", numpy array of shape (m, n_x)\n",
    "# this is the shape of the first input!\n",
    "x[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the architecture and weights with hidden size n_a \n",
    "n_a = \n",
    "\n",
    "# Weight matrix multiplying the hidden state\n",
    "Waa = np.random.uniform(size=())\n",
    "\n",
    "# Weight matrix multiplying the input x\n",
    "Wax = np.random.uniform(size=())\n",
    "\n",
    "# Weight matrix multiplying the predictions y\n",
    "Wya = np.random.uniform(size=())\n",
    "\n",
    "a0 = np.zeros([])\n",
    "a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one iteration\n",
    "a1 = np.zeros()\n",
    "\n",
    "# calculate the value of prediction\n",
    "y1 ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop through time\n",
    "\n",
    "Backpropagating error requires error to flow backwards in time\n",
    "- error must flow back to the first time step to calculate gradients\n",
    "\n",
    "The loss function for a given layer depends not only on its infulence on layers below it - but also on the layer at the next time step\n",
    "\n",
    "Backproping through time means unrolling, which makes\n",
    "-  the memory footprint of recurrent neural network large\n",
    "- parallel training on multiple sequences inefficient on hardware that shares memory (i.e. GPU)\n",
    "\n",
    "Further reading - see *Truncated Backprop Through Time*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical\n",
    "\n",
    "Let's exercise the concepts by programming a recurrent neural network.\n",
    "More specifically, let's create a character level language model to predict next character in a word.\n",
    "\n",
    "### Character level language modeling\n",
    "\n",
    "For this first exercise we will generate a \"dummy\" training set, consisting of the same *one* word repeated many times.\n",
    "\n",
    "And the model will be able to only predict the next letter of this word.\n",
    "\n",
    "+ many-to-many model\n",
    "+ Feeding in the entire input sequence then reading the output sequence (the next letter)\n",
    "\n",
    "Later, after implementing all of these steps of data preparation for a \"dummy\" training set, you can re-use it for something more interesting by reading a text from some file as training set.\n",
    "\n",
    "<img src=\"assets/character_model.png\" alt=\"\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "1. read the word and encode it into one-hot vectors\n",
    "    1. create list of unique lower case chars from the word\n",
    "    1. create lookup dictionaries to map each char to its int and the other way around too\n",
    "    1. encode from chars to ints: generate sequence of numbers from the characters of the word\n",
    "    1. transform the sequence of numbers to one-hot vectors\n",
    "    \n",
    "1. generate the \"dummy training set\"\n",
    "    1. repeat the one-hot-vectors of the word 500 times\n",
    "    1. split that into batches: sequences of input-output (or feature-target) with Input size = how many letters in each feature. Output size will be the same as input size. The output is always the input shifted by one letter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 read the word and encode it into one-hot vectors\n",
    "# 1A create list of unique lower case chars from the word\n",
    "\n",
    "def encode(unique_chars, string):\n",
    "    '''\n",
    "    Creates one-hot encode vector of the string.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    unique_chars: list with unique characters.\n",
    "    string: string to be one-hot encoded.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    array of one-hot encoded characters.\n",
    "    '''\n",
    "    # 1B create lookup dictionaries to map each char to its int\n",
    "    char_to_int = \n",
    "    \n",
    "    # 1C encode from chars to ints: generate sequence of numbers from the characters of the word\n",
    "    string_as_int = \n",
    "    \n",
    "    # 1D transform the sequence of numbers to one-hot vectors\n",
    "    return keras.utils.to_categorical()\n",
    "\n",
    "\n",
    "text ='goodbye'\n",
    "characters = \n",
    "encode(characters, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the \"dummy\" training set\n",
    "batch_size = 100\n",
    "seq_length = 3\n",
    "\n",
    "\n",
    "def make_batches(training_data, unique_chars, batch_size, seq_len):\n",
    "    '''\n",
    "    Generates random sequences of size `seq_len` from the `training_data`, as tuples of input-output, or feature-target. of the model.\n",
    "     \n",
    "    Parameters:\n",
    "    --------\n",
    "    training_data: string. What will be used to train the model.\n",
    "    unique_chars: List with unique characters.\n",
    "    batch_size: int. How many sequences per batch. \n",
    "    seq_len: int. How many encoded chars in a sequence.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    input: array. Each object in the array is a feature: \n",
    "            a sequence (vector of size `seq_len`) of chars from the `training_data`, encoded as one-hot vectors. \n",
    "    output: array. Each object in the array is a target: \n",
    "            a sequence (vector of size `seq_len`) of the same chars from the `input`, shifted by one and encoded as one-hot vectors. \n",
    "    '''\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode: one-hot vectors to string\n",
    "def decode(alphabet, encoded):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the model\n",
    "\n",
    "Your model will have the following structure: \n",
    "\n",
    "- Initialize parameters \n",
    "- Input layer takes sequence with `seq_len` encoded chars in a sequence and features being unique characters from the training set\n",
    "- RNN layer with 15 memory cells\n",
    "\n",
    "At each time-step, the RNN tries to predict what are the next characters given the previous characters. \n",
    "\n",
    "The dataset $X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ is a list of characters in the training set, while $Y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ is such that at every time-step $t$, we have $y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:56:56.625061Z",
     "start_time": "2019-09-30T15:56:56.562665Z"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.add(keras.layers.SimpleRNN(15, return_sequences=True))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adadelta(),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "h = model.fit(batches, epochs=3, verbose=0)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:56:58.100372Z",
     "start_time": "2019-09-30T15:56:58.053575Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:56:59.229302Z",
     "start_time": "2019-09-30T15:56:59.208302Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequences\n",
    "\n",
    "A simple example of sequence prediction - $[0, 1, 2] \\rightarrow [3, 4, 5]$\n",
    "\n",
    "Application examples:\n",
    "![](assets/application_examples.jpg)\n",
    "\n",
    "We can have different types of sequence problem structures\n",
    "\n",
    "![](assets/sequences.png)\n",
    "\n",
    "The many to many structure can also be thought of as an encoder-decoder structure:\n",
    "\n",
    "![](assets/quoc-le.png)\n",
    "\n",
    "## Problems with standard dense networks\n",
    "\n",
    "+ Fixed size inputs & outputs\n",
    "+ Stateless\n",
    "+ Doesn't share features learned across positions\n",
    "+ Unaware of temporal structure\n",
    "\n",
    "\n",
    "## Promise of recurrent neural networks\n",
    "\n",
    "Network able to learn a mapping from inputs over time\n",
    "- outputs become conditional the context of the sequence\n",
    "\n",
    "Learn the temporal dependence of data\n",
    "\n",
    "An RRN is Turing complete\n",
    "- they can simulate arbitrary programs\n",
    "\n",
    "## Being comfortable in three dimensions\n",
    "\n",
    "We model the temporal structure in data using a dimension in an array - by convention this is the second dimension.\n",
    "\n",
    "Our dimensions then are: \n",
    "- $m$ = the batch dimension (number of samples)\n",
    "- $T_x$ = timesteps (length of sequence)\n",
    "- $n_x$ = features at each time-step\n",
    "\n",
    "## Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:50:01.190888Z",
     "start_time": "2019-09-30T15:49:56.813545Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 32, 16])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow import random\n",
    "random.set_seed(3) # for reproducible results\n",
    "\n",
    "\n",
    "m = 1000\n",
    "T_x = 32\n",
    "n_x = 16\n",
    "\n",
    "samples = tf.random.uniform((m, T_x, n_x))\n",
    "\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select all samples, first timestep, all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.numpy()[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last sample, all timesteps, first feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.numpy()[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ninth sample, sixth timestep, all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.numpy()[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "A recurrent neural network is a linear stack of the one same model. It passes its output to itself at each timestep.\n",
    "![](assets/RNN.png)\n",
    "\n",
    "### RNN cell\n",
    "![](assets/rnn_step_forward.png)\n",
    "\n",
    "Basic RNN cell. Takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (activation value or previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $y^{\\langle t \\rangle}$\n",
    "\n",
    "### RNN forward pass \n",
    "\n",
    "You can see an RNN as the repetition of the cell you've just built. If your input sequence of data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell ($a^{\\langle t-1 \\rangle}$) and the current time-step's input data ($x^{\\langle t \\rangle}$). It outputs a new hidden state ($a^{\\langle t \\rangle}$) and a prediction ($y^{\\langle t \\rangle}$) for this time-step.\n",
    "\n",
    "\n",
    "![](assets/rnn.png)\n",
    "Basic RNN. The input sequence $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  is carried over $T_x$ time steps. The network outputs $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical \n",
    "\n",
    "Let's code a forward propagation similar to the RNN described in the figure above, but simpler, that is without biases.\n",
    "We also want to have one output for each time step, that means  $T_x$ =  $T_y$\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Create a random input vector x with 4 samples, 3 time-steps and 2 features at each time-step  \n",
    "+ Initialize the architecture and weights with hidden size = 4 \n",
    "+ Create the hidden state vectors ($a$) as a vector of zeros that will store the values computer by RNN\n",
    "+ Loop over each time-step (index t)\n",
    "    + Calculate the \"next\" hidden state using tanh as activation function\n",
    "    + Calculate the prediciton y in this time-step\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.88571262, 0.59926573],\n",
       "        [0.47102437, 0.55445859],\n",
       "        [0.44981887, 0.09304059]],\n",
       "\n",
       "       [[0.81935112, 0.90132869],\n",
       "        [0.47578489, 0.96259905],\n",
       "        [0.82384898, 0.45511733]],\n",
       "\n",
       "       [[0.01477745, 0.35827748],\n",
       "        [0.37231846, 0.68358015],\n",
       "        [0.32437978, 0.41808654]],\n",
       "\n",
       "       [[0.56037974, 0.02307087],\n",
       "        [0.71701891, 0.03027807],\n",
       "        [0.20014658, 0.7836883 ]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random input vector x with m samples,T_x time-steps and n_x features\n",
    "m = \n",
    "T_x = \n",
    "n_x = \n",
    "\n",
    "x = np.random.uniform(size=(m, T_x, n_x))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xt  -- your input data at timestep \"t\", numpy array of shape (m, n_x)\n",
    "# this is the shape of the first input!\n",
    "x[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the architecture and weights with hidden size n_a \n",
    "n_a = \n",
    "\n",
    "# Weight matrix multiplying the hidden state\n",
    "Waa = np.random.uniform(size=())\n",
    "\n",
    "# Weight matrix multiplying the input x\n",
    "Wax = np.random.uniform(size=())\n",
    "\n",
    "# Weight matrix multiplying the predictions y\n",
    "Wya = np.random.uniform(size=())\n",
    "\n",
    "a0 = np.zeros([])\n",
    "a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one iteration\n",
    "a1 = np.zeros()\n",
    "\n",
    "# calculate the value of prediction\n",
    "y1 ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop through time\n",
    "\n",
    "Backpropagating error requires error to flow backwards in time\n",
    "- error must flow back to the first time step to calculate gradients\n",
    "\n",
    "The loss function for a given layer depends not only on its infulence on layers below it - but also on the layer at the next time step\n",
    "\n",
    "Backproping through time means unrolling, which makes\n",
    "-  the memory footprint of recurrent neural network large\n",
    "- parallel training on multiple sequences inefficient on hardware that shares memory (i.e. GPU)\n",
    "\n",
    "Further reading - see *Truncated Backprop Through Time*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra\n",
    "\n",
    "Exercise about Character level language modeling to predict next character in the word goodbye from Adam's notebook [link](https://github.com/ADGEfficiency/teaching-monolith/blob/master/sequences/recurrent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

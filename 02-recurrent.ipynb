{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequences\n",
    "\n",
    "A simple example of sequence prediction - $[0, 1, 2] \\rightarrow [3, 4, 5]$\n",
    "\n",
    "Application examples:\n",
    "![](assets/application_examples.jpg)\n",
    "\n",
    "We can have different types of sequence problem structures\n",
    "\n",
    "![](assets/sequences.png)\n",
    "\n",
    "The many to many structure can also be thought of as an encoder-decoder structure:\n",
    "\n",
    "![](assets/quoc-le.png)\n",
    "\n",
    "## Problems with standard dense networks\n",
    "\n",
    "+ Fixed size inputs & outputs\n",
    "+ Stateless\n",
    "+ Doesn't share features learned across positions\n",
    "+ Unaware of temporal structure\n",
    "\n",
    "\n",
    "## Promise of recurrent neural networks\n",
    "\n",
    "Network able to learn a mapping from inputs over time\n",
    "- outputs become conditional the context of the sequence\n",
    "\n",
    "Learn the temporal dependence of data\n",
    "\n",
    "An RRN is Turing complete\n",
    "- they can simulate arbitrary programs\n",
    "\n",
    "## Being comfortable in three dimensions\n",
    "\n",
    "We model the temporal structure in data using a dimension in an array - by convention this is the second dimension.\n",
    "\n",
    "Our dimensions then are: \n",
    "- m = the batch dimension (number of samples)\n",
    "- T_x = timesteps (length of sequence)\n",
    "- n_x = features at each time-step\n",
    "\n",
    "## Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:50:01.190888Z",
     "start_time": "2019-09-30T15:49:56.813545Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 32, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "m = 1000\n",
    "T_x = 32\n",
    "n_x = 16\n",
    "\n",
    "samples = tf.random.uniform((m, T_x, n_x))\n",
    "\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select all samples, first timestep, all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:56:27.901385Z",
     "start_time": "2019-09-30T15:56:27.822588Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last sample, all timesteps, first feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:56:12.623696Z",
     "start_time": "2019-09-30T15:56:12.544701Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ninth sample, sixth timestep, all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T08:49:37.865647Z",
     "start_time": "2019-10-01T08:49:37.736849Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "A recurrent neural network is a linear stack of the one same model. It passes its output to itself at each timestep.\n",
    "![](assets/RNN.png)\n",
    "\n",
    "### RNN cell\n",
    "![](assets/rnn_step_forward.png)\n",
    "\n",
    "Basic RNN cell. Takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (activation value or previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $y^{\\langle t \\rangle}$\n",
    "\n",
    "### RNN forward pass \n",
    "\n",
    "You can see an RNN as the repetition of the cell you've just built. If your input sequence of data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell ($a^{\\langle t-1 \\rangle}$) and the current time-step's input data ($x^{\\langle t \\rangle}$). It outputs a hidden state ($a^{\\langle t \\rangle}$) and a prediction ($y^{\\langle t \\rangle}$) for this time-step.\n",
    "\n",
    "\n",
    "![](assets/rnn.png)\n",
    "Basic RNN. The input sequence $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  is carried over $T_x$ time steps. The network outputs $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code a forward propagation similar to the RNN described in the figure above, but simpler: without biases.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Create a random input vector x with 4 samples, 3 time-steps and 2 features at each time-step  \n",
    "+ Initialize the architecture and weights with 4 hidden layers\n",
    "+ Create a vector of zeros ( a ) to store the hidden state computed by the RNN\n",
    "+ Loop over each time-step (index t)\n",
    "    + Calculate the \"next\" hidden state using tanh as activation function\n",
    "    + Calculate the prediciton y in this time-step\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop through time\n",
    "\n",
    "Backpropagating error requires error to flow backwards in time\n",
    "- error must flow back to the first time step to calculate gradients\n",
    "\n",
    "The loss function for a given layer depends not only on its infulence on layers below it - but also on the layer at the next time step\n",
    "\n",
    "Backproping through time means unrolling, which makes\n",
    "-  the memory footprint of recurrent neural network large\n",
    "- parallel training on multiple sequences inefficient on hardware that shares memory (i.e. GPU)\n",
    "\n",
    "Further reading - see *Truncated Backprop Through Time*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Character level language modeling\n",
    "\n",
    "Lets use a recurrent neural network to predict the next letter in the word *goodbye!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ many-to-many model\n",
    "+ Feeding in the entire input sequence then reading the output sequence\n",
    "\n",
    "<img src=\"assets/character_model.png\" alt=\"\" width=\"500\"/>\n",
    "\n",
    "### Data preparation\n",
    "+ create a vocabulary using unique lower case letters\n",
    "+ create a index dictionaty to map each char to its index and the other way around too\n",
    "+ generate sequences\n",
    "+ transform sequences to one-hot vectors\n",
    "\n",
    "### Overview of the model\n",
    "\n",
    "Your model will have the following structure: \n",
    "\n",
    "- Initialize parameters \n",
    "- Input layer takes sequence with `len_seq` time steps and `vocab_size` features\n",
    "- RNN layer with 15 memory cells\n",
    "\n",
    "At each time-step, the RNN tries to predict what is the next character given the previous characters. The dataset $X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ is a list of characters in the training set, while $Y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ is such that at every time-step $t$, we have $y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:56:50.124857Z",
     "start_time": "2019-09-30T15:56:50.078060Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(alphabet, samples, seq_len):\n",
    "    '''\n",
    "    Creates one-hot encode vector of the sequence of characters.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    alphabet: list with unique characters.\n",
    "    samples: string of characters to be one-hot encoded.\n",
    "    seq_len: int.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    one_hot: array of one-hot encoded characters.\n",
    "    '''\n",
    "\n",
    "\n",
    "def make_dataset(word, seq_len):\n",
    "    '''\n",
    "    Generates random sequences of size `seq_len` from the base `word`, as tuples of input-output one-hot encoded.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    word: string. Word used to train the model. It will be repeated 50 times.\n",
    "    seq_len: int. How many characters to consider in the input sequence. \n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    input: array of size 100. Each object in the array is a random sample from the base `word` with size `seq_len`. \n",
    "    output: array of size 100.  Each object in the array is the sequence of size `seq_len` from the sample in the input.\n",
    "    alphabet: list with unique characters from parameter `word`.\n",
    "    '''\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:56:53.876830Z",
     "start_time": "2019-09-30T15:56:52.512910Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:56:56.625061Z",
     "start_time": "2019-09-30T15:56:56.562665Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode(alphabet, encoded):\n",
    "    #  single sample only\n",
    "    return\n",
    "\n",
    "test = encode(alphabet, np.array(['goo']), seq_len=3)\n",
    "decode(alphabet, np.argmax(model.predict(test), axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:56:58.100372Z",
     "start_time": "2019-09-30T15:56:58.053575Z"
    }
   },
   "outputs": [],
   "source": [
    "test = encode(alphabet, np.array(['bye']), seq_len=3)\n",
    "decode(alphabet, np.argmax(model.predict(test), axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:56:59.229302Z",
     "start_time": "2019-09-30T15:56:59.208302Z"
    }
   },
   "outputs": [],
   "source": [
    "test = encode(alphabet, np.array(['!go']), seq_len=3)\n",
    "decode(alphabet, np.argmax(model.predict(test), axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical\n",
    "\n",
    "Character level language model to predict next character in a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
